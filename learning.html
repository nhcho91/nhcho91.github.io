<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Machine Learning</title>
<!-- MathJax -->
<!-- previous CDN source: <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({	  TeX: { equationNumbers: { autoNumber: "AMS" } } }); </script> -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Namhoon Cho</div>
<div class="menu-item"><a href="index.html">About&nbsp;Me</a></div>
<div class="menu-item"><a href="https://www.dropbox.com/sh/66be7tirhr56y2p/AAAKsW7RRNf9N_ABcqQgcO8Na?preview=CV_NCho_full.pdf" target="blank">CV</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="interests.html">Interests</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="software.html">Software</a></div>
<div class="menu-item"><a href="multimedia.html">Multimedia</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="students.html">Students</a></div>
<div class="menu-item"><a href="courses.html">Courses</a></div>
<div class="menu-category">Collections</div>
<div class="menu-item"><a href="control.html">Control</a></div>
<div class="menu-item"><a href="learning.html" class="current">Machine&nbsp;Learning</a></div>
<div class="menu-item"><a href="music.html">Music</a></div>
<div class="menu-item"><a href="https://nhcho91.github.io/control-deadlines" target="blank">Deadlines</a></div>
<div class="menu-category">Collaborators</div>
<div class="menu-item"><a href="https://www.cranfield.ac.uk/centres/centre-for-autonomous-and-cyberphysical-systems" target="blank">Cranfield&nbsp;CACPS</a></div>
<div class="menu-item"><a href="http://fdcl.snu.ac.kr" target="blank">SNU&nbsp;FDCL</a></div>
<div class="menu-item"><a href="https://sites.google.com/view/fdcl-kaist" target="blank">KAIST&nbsp;FDCL</a></div>
<div class="menu-item"><a href="https://www.add.re.kr/eps" target="blank">ADD</a></div>
<div class="menu-item"><a href="https://https://sites.google.com/view/kaist-ais" target="blank">Hyo-Sang&nbsp;Shin</a></div>
<div class="menu-item"><a href="https://cau-aisl.github.io">Seokwon&nbsp;Lee</a></div>
<div class="menu-item"><a href="https://sites.google.com/view/fdcl-kaist" target="blank">Chang-Hun&nbsp;Lee</a></div>
<div class="menu-item"><a href="https://sites.google.com/ajou.ac.kr/parkjo05/jongho-park" target="blank">Jongho&nbsp;Park</a></div>
<div class="menu-item"><a href="https://sites.google.com/view/mingukim" target="blank">Mingu&nbsp;Kim</a></div>
<div class="menu-item"><a href="https://fmcl.kookmin.ac.kr" target="blank">Suwon&nbsp;Lee</a></div>
<div class="menu-item"><a href="https://www.notion.so/Jinrae-Kim-00e9f9ff9fec4d329a18642f9c31e3e0" target="blank">Jinrae&nbsp;Kim</a></div>
<div class="menu-item"><a href="https://www.tudelft.nl/staff/s.baldi/" target="blank">Simone&nbsp;Baldi</a></div>
<div class="menu-item"><a href="https://www.imperial.ac.uk/people/d.amato" target="blank">Davide&nbsp;Amato</a></div>
<div class="menu-item"><a href="https://azizan.mit.edu" target="blank">Navid&nbsp;Azizan</a></div>
<div class="menu-item"><a href="https://youngjae-min.github.io" target="blank">Youngjae&nbsp;Min</a></div>
<div class="menu-item"><a href="https://jay-choi.me" target="blank">Jason&nbsp;J.&nbsp;Choi</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?hl=en&user=Fcs6aHgAAAAJ">Christopher&nbsp;A.&nbsp;Strong</a></div>
<div class="menu-item"><a href="https://taha.eng.uci.edu" target="blank">Haithem&nbsp;E.&nbsp;Taha</a></div>
<div class="menu-item"><a href="https://sites.google.com/view/taeyoon-lee" target="blank">Taeyoon&nbsp;Lee</a></div>
<div class="menu-item"><a href="https://engineering.purdue.edu/OguriGroup" target="blank">Kenshiro&nbsp;Oguri</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Machine Learning</h1>
</div>
<h2>Bayesian Framework</h2>
<ul>
<li><p><a href="https://doi.org/10.1162/089976699300016674" target=&ldquo;blank&rdquo;>1999 - Sam Roweis - A Unifying Review of Linear Gaussian Models</a></p>
</li>
<li><p><a href="https://tminka.github.io/papers/ep/" target=&ldquo;blank&rdquo;>2001 - Thomas Minka - A Family of Algorithms for Approximate Bayesian Inference</a></p>
</li>
<li><p><a href="https://www.cs.ubc.ca/~murphyk/Thesis/thesis.html" target=&ldquo;blank&rdquo;>2002 - Kevin Murphy - Dynamic Bayesian Networks: Representation, Inference and Learning</a></p>
</li>
<li><p><a href="https://tminka.github.io/papers/message-passing/" target=&ldquo;blank&rdquo;>2005 - Thomas Minka - Divergence Measures and Message Passing</a></p>
</li>
<li><p><a href="https://tminka.github.io/papers/dynamic.html" target=&ldquo;blank&rdquo;>2007 - Thomas Minka - Bayesian Inference in Dynamic Models - An Overview</a></p>
</li>
<li><p><a href="https://doi.org/10.1098/rsta.2011.0553" target=&ldquo;blank&rdquo;>2013 - Zoubin Ghahramani - Bayesian Non-parametrics and the Probabilistic Approach to Modelling</a></p>
</li>
<li><p><a href="https://mlg-blog.com/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-1.html" target=&ldquo;blank&rdquo;>2021 - Wessel Bruinsma - What Keeps a Bayesian Awake at Night? Part 1: Day Time</a>, <a href="https://mlg-blog.com/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-2.html" target=&ldquo;blank&rdquo;>Part 2: Night Time</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.04562" target=&ldquo;blank&rdquo;>2021 - Mohammad Emtiyaz Khan - The Bayesian Learning Rule</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.08769" target=&ldquo;blank&rdquo;>2021 - Mohammad Emtiyaz Khan - Knowledge-Adaptation Priors</a></p>
</li>
</ul>
<p><br /></p>
<h2>Gaussian Processes</h2>
<h3>Introductory Materials</h3>
<ul>
<li><p><a href="https://towardsdatascience.com/understanding-gaussian-process-the-socratic-way-ba02369d804?sk=2bd38bef8f815b36596c44fa1cbfdd1f" target=&ldquo;blank&rdquo;>2019 - Wei Yi - Understanding Gaussian Process, the Socratic Way</a></p>
</li>
<li><p><a href="http://tinyurl.com/guide2gp" target=&ldquo;blank&rdquo;>2020 - Marc Peter Deisenroth - A Practical Guide to Gaussian Processes</a></p>
</li>
</ul>
<h3>Efficient Online Learning and Inference</h3>
<ul>
<li><p><a href="http://www.cs.ubbcluj.ro/~csatol/publications/thesis.pdf" target=&ldquo;blank&rdquo;>2002 - Lehel Csato - Gaussian Processes - Iterative Sparse Approximations</a></p>
</li>
<li><p><a href="https://www.jmlr.org/papers/v18/16-603.html" target=&ldquo;blank&rdquo;>2017 - Thang D. Bui - A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</a></p>
</li>
<li><p><a href="https://neurips.cc/Conferences/2017/Schedule?showEvent=9114" target=&ldquo;blank&rdquo;>2017 - Thang D. Bui - Streaming Sparse Gaussian Process Approximations</a>
</p>
</li>
<li><p><a href="https://jmlr.org/papers/v22/20-1260.html" target=&ldquo;blank&rdquo;>2021 - James T. Wilson - Pathwise Conditioning of Gaussian Processes</a></p>
</li>
</ul>
<p><br /></p>
<h2>Information Geometry</h2>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2013/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html" target=&ldquo;blank&rdquo;>2013 - Marco Cuturi - Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a></p>
</li>
<li><p><a href="https://doi.org/10.1109/TIT.2015.2388583" target=&ldquo;blank&rdquo;>2015 - Garvesh Raskutti - The Information Geometry of Mirror Descent</a></p>
</li>
<li><p><a href="https://doi.org/10.1090/noti1647" target=&ldquo;blank&rdquo;>2018 - Frank Nielson - What is&hellip; an Information Projection?</a></p>
</li>
<li><p><a href="https://doi.org/10.3390/e22101100" target=&ldquo;blank&rdquo;>2020 - Frank Nielson - An Elementary Introduction to Information Geometry</a></p>
</li>
</ul>
<p><br /></p>
<h2>Optimisation for Machine Learning</h2>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html" target=&ldquo;blank&rdquo;>2007 - Ali Rahimi - Random Features for Large-Scale Kernel Machines</a></p>
</li>
<li><p><a href="https://papers.nips.cc/paper/2008/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html" target=&ldquo;blank&rdquo;>2008 - Ali Rahimi - Weighted Sums of Random Kitchen Sinks: Replacing Minimization with Randomization in Learning</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1412.6980" target=&ldquo;blank&rdquo;>2015 - Diederik P. Kingma - Adam: A Method for Stochastic Optimization</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ" target=&ldquo;blank&rdquo;>2016 - Timothy Dozat - Incorporating Nesterov Momentum into Adam</a></p>
</li>
<li><p><a href="https://doi.org/10.1073/pnas.1614734113" target=&ldquo;blank&rdquo;>2016 - Andre Wibisono - A Variational Perspective on Accelerated Methods in Optimization</a></p>
</li>
<li><p><a href="https://icml.cc/Conferences/2017/ScheduleMultitrack?event=1144" target=&ldquo;blank&rdquo;>2017 - Chelsea Finn - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></p>
</li>
<li><p><a href="https://papers.nips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html" target=&ldquo;blank&rdquo;>2018 - Ricky T. Q. Chen - Neural Ordinary Differential Equations</a></p>
</li>
<li><p><a href="https://doi.org/10.1109/CDC40024.2019.9029197" target=&ldquo;blank&rdquo;>2019 - Joseph E. Gaudio - Connections Between Adaptive Control and Optimization in Machine Learning</a></p>
</li>
<li><p><a href="https://doi.org/10.1137/18M1172314" target=&ldquo;blank&rdquo;>2019 -  Jelena Diakonikolas - The Approximate Duality Gap Technique: A Unified Theory of First-Order Methods</a></p>
</li>
<li><p><a href="https://doi.org/10.1109/LCSYS.2020.3002513" target=&ldquo;blank&rdquo;>2021 - Joseph E. Gaudio - A Class of High Order Tuners for Adaptive Systems</a></p>
</li>
<li><p><a href="https://papers.nips.cc/paper/2021/hash/647c722bf90a49140184672e0d3723e3-Abstract.html" target=&ldquo;blank&rdquo;>2021 - Jongmin Lee - A Geometric Structure of Acceleration and Its Role in Making Gradients Small Fast</a></p>
</li>
<li><p><a href="https://proceedings.mlr.press/v162/suh22a.html" target=&ldquo;blank&rdquo;>2022 - Jaewook J. Suh - Continuous-Time Analysis of Accelerated Gradient Methods via Conservation Laws in Dilated Coordinate Systems</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.05187" target=&ldquo;blank&rdquo;>2023 - Jeremy Bernstein - Automatic Gradient Descent: Deep Learning without Hyperparameters</a></p>
</li>
<li><p><a href="https://proceedings.mlr.press/v202/defazio23a.html" target=&ldquo;blank&rdquo;>2023 - Aaron Defazio - Learning-Rate-Free Learning by D-Adaptation</a></p>
</li>
<li><p><a href="https://doi.org/10.1007/s00245-023-10047-9" target=&ldquo;blank&rdquo;>2023 - Chanwoo Park - Factor-\(\sqrt{2}\) Acceleration of Accelerated Gradient Methods</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.06628" target=&ldquo;blank&rdquo;>2023 - Jaeyeon Kim - Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2311.17296" target=&ldquo;blank&rdquo;>2023 - Jaeyeon Kim - Mirror Duality in Convex Optimization</a></p>
</li>
</ul>
<p><br /></p>
<h2>Neural Network Architecture</h2>
<ul>
<li><p><a href="https://doi.org/10.1038/s42256-020-00237-3" target=&ldquo;blank&rdquo;>2020 - Mathias Lechner - Neural Circuit Policies Enabling Auditable Autonomy</a></p>
</li>
<li><p><a href="https://doi.org/10.1109/LCSYS.2020.3038221" target=&ldquo;blank&rdquo;>2021 - Max Revay - A Convex Parameterization of Robust Recurrent Neural Networks</a></p>
</li>
<li><p><a href="https://doi.org/10.1038/s42256-022-00556-7" target=&ldquo;blank&rdquo;>2022 - Ramin Hasani - Closed-Form Continuous-Time Neural Networks</a></p>
</li>
<li><p><a href="https://doi.org/10.1109/TNNLS.2022.3190198" target=&ldquo;blank&rdquo;>2022 - Jinrae Kim - Parameterized Convex Universal Approximators for Decision-Making Problems</a></p>
</li>
<li><p><a href="https://proceedings.mlr.press/v202/wang23v.html" target=&ldquo;blank&rdquo;>2023 - Ruigang Wang - Direct Parameterization of Lipschitz-Bounded Deep Networks</a></p>
</li>
<li><p><a href="https://doi.org/10.1109/TAC.2023.3294101" target=&ldquo;blank&rdquo;>2023 - Max Revay - Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2410.10807" target=&ldquo;blank&rdquo;>2024 - Youngjae Min - Hard-Constrained Neural Networks with Universal Approximation Guarantees</a></p>
</li>
<li><p>[https:<i></i>doi.org<i>10.1109</i>CDC56724.2024.10886407 2024 - Jing Cheng - Learning Stable and Passive Neural Differential Equations</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-10-11, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
