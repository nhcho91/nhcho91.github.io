# jemdoc: menu{MENU}{learning.html}, notime
= Machine Learning

== Bayesian Framework
- [https://doi.org/10.1162/089976699300016674 1999 - Sam Roweis - A Unifying Review of Linear Gaussian Models]
- [https://tminka.github.io/papers/ep/ 2001 - Thomas Minka - A Family of Algorithms for Approximate Bayesian Inference]
- [https://www.cs.ubc.ca/~murphyk/Thesis/thesis.html 2002 - Kevin Murphy - Dynamic Bayesian Networks: Representation, Inference and Learning]
- [https://tminka.github.io/papers/message-passing/ 2005 - Thomas Minka - Divergence Measures and Message Passing]
- [https://tminka.github.io/papers/dynamic.html 2007 - Thomas Minka - Bayesian Inference in Dynamic Models - An Overview]
- [https://doi.org/10.1098/rsta.2011.0553 2013 - Zoubin Ghahramani - Bayesian Non-parametrics and the Probabilistic Approach to Modelling]
- [https://mlg-blog.com/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-1.html 2021 - Wessel Bruinsma - What Keeps a Bayesian Awake at Night? Part 1: Day Time], [https://mlg-blog.com/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-2.html Part 2: Night Time]
- [https://arxiv.org/abs/2107.04562 2021 - Mohammad Emtiyaz Khan - The Bayesian Learning Rule]
- [https://arxiv.org/abs/2106.08769 2021 - Mohammad Emtiyaz Khan - Knowledge-Adaptation Priors]

\n
== Gaussian Processes
=== Introductory Materials
- [https://towardsdatascience.com/understanding-gaussian-process-the-socratic-way-ba02369d804?sk=2bd38bef8f815b36596c44fa1cbfdd1f 2019 - Wei Yi - Understanding Gaussian Process, the Socratic Way]
- [http://tinyurl.com/guide2gp 2020 - Marc Peter Deisenroth - A Practical Guide to Gaussian Processes]

=== Efficient Online Learning and Inference
- [http://www.cs.ubbcluj.ro/~csatol/publications/thesis.pdf 2002 - Lehel Csato - Gaussian Processes - Iterative Sparse Approximations]
- [https://www.jmlr.org/papers/v18/16-603.html 2017 - Thang D. Bui - A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation]
- [https://neurips.cc/Conferences/2017/Schedule?showEvent=9114 2017 - Thang D. Bui - Streaming Sparse Gaussian Process Approximations]
#- [http://mlg.eng.cam.ac.uk/thang/docs/papers/thesis-thang.pdf 2017 - Thang D. Bui - Efficient Deterministic Approximate Bayesian Inference for Gaussian Process Models]
- [https://jmlr.org/papers/v22/20-1260.html 2021 - James T. Wilson - Pathwise Conditioning of Gaussian Processes]

\n
== Information Geometry
- [https://papers.nips.cc/paper/2013/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html 2013 - Marco Cuturi - Sinkhorn Distances: Lightspeed Computation of Optimal Transport]
- [https://doi.org/10.1109/TIT.2015.2388583 2015 - Garvesh Raskutti - The Information Geometry of Mirror Descent]
- [https://doi.org/10.1090/noti1647 2018 - Frank Nielson - What is... an Information Projection?]
- [https://doi.org/10.3390/e22101100 2020 - Frank Nielson - An Elementary Introduction to Information Geometry]

\n
== Optimisation for Machine Learning
- [https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html 2007 - Ali Rahimi - Random Features for Large-Scale Kernel Machines]
- [https://papers.nips.cc/paper/2008/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html 2008 - Ali Rahimi - Weighted Sums of Random Kitchen Sinks: Replacing Minimization with Randomization in Learning]
- [https://arxiv.org/abs/1412.6980 2015 - Diederik P. Kingma - Adam: A Method for Stochastic Optimization]
- [https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ 2016 - Timothy Dozat - Incorporating Nesterov Momentum into Adam]
- [https://doi.org/10.1073/pnas.1614734113 2016 - Andre Wibisono - A Variational Perspective on Accelerated Methods in Optimization]
- [https://icml.cc/Conferences/2017/ScheduleMultitrack?event=1144 2017 - Chelsea Finn - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks]
- [https://papers.nips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html 2018 - Ricky T. Q. Chen - Neural Ordinary Differential Equations]
- [https://doi.org/10.1109/CDC40024.2019.9029197 2019 - Joseph E. Gaudio - Connections Between Adaptive Control and Optimization in Machine Learning]
- [https://doi.org/10.1137/18M1172314 2019 -  Jelena Diakonikolas - The Approximate Duality Gap Technique: A Unified Theory of First-Order Methods]
- [https://doi.org/10.1109/LCSYS.2020.3002513 2021 - Joseph E. Gaudio - A Class of High Order Tuners for Adaptive Systems]
- [https://papers.nips.cc/paper/2021/hash/647c722bf90a49140184672e0d3723e3-Abstract.html 2021 - Jongmin Lee - A Geometric Structure of Acceleration and Its Role in Making Gradients Small Fast]
- [https://proceedings.mlr.press/v162/suh22a.html 2022 - Jaewook J. Suh - Continuous-Time Analysis of Accelerated Gradient Methods via Conservation Laws in Dilated Coordinate Systems]
- [https://arxiv.org/abs/2304.05187 2023 - Jeremy Bernstein - Automatic Gradient Descent: Deep Learning without Hyperparameters]
- [https://proceedings.mlr.press/v202/defazio23a.html 2023 - Aaron Defazio - Learning-Rate-Free Learning by D-Adaptation]
- [https://doi.org/10.1007/s00245-023-10047-9 2023 - Chanwoo Park - Factor-$\sqrt{2}$ Acceleration of Accelerated Gradient Methods]
- [https://arxiv.org/abs/2305.06628 2023 - Jaeyeon Kim - Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value]
- [https://arxiv.org/abs/2311.17296 2023 - Jaeyeon Kim - Mirror Duality in Convex Optimization]

\n
== Neural Network Architecture
- [https://doi.org/10.1038/s42256-020-00237-3 2020 - Mathias Lechner - Neural Circuit Policies Enabling Auditable Autonomy]
- [https://doi.org/10.1109/LCSYS.2020.3038221 2021 - Max Revay - A Convex Parameterization of Robust Recurrent Neural Networks]
- [https://doi.org/10.1038/s42256-022-00556-7 2022 - Ramin Hasani - Closed-Form Continuous-Time Neural Networks]
- [https://doi.org/10.1109/TNNLS.2022.3190198 2022 - Jinrae Kim - Parameterized Convex Universal Approximators for Decision-Making Problems]
- [https://proceedings.mlr.press/v202/wang23v.html 2023 - Ruigang Wang - Direct Parameterization of Lipschitz-Bounded Deep Networks]
- [https://doi.org/10.1109/TAC.2023.3294101 2023 - Max Revay - Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness]
- [https://arxiv.org/abs/2410.10807 2024 - Youngjae Min - Hard-Constrained Neural Networks with Universal Approximation Guarantees]
- [https://doi.org/10.1109/CDC56724.2024.10886407 2024 - Jing Cheng - Learning Stable and Passive Neural Differential Equations]